{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6J8EiolcK1YU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 86.3MB 862kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/2d/2fe51d8382994cc0d4f9734367e8c159808ef2c367c6672722a509c9d5b2/grpcio-1.24.1-cp37-cp37m-manylinux1_x86_64.whl (2.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 23.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K    100% |████████████████████████████████| 450kB 59.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 50.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.16.2)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 59.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 31.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 49.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting keras-applications>=1.0.8 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 42.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.8MB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.1)\n",
      "Requirement already satisfied: wheel>=0.26 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.1)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/34/02a2083afc14adff644a1e29783f276f12f1f914ca4cab157d73bb3d2fed/protobuf-3.10.0-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 43.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.14.1)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/9a/50fadfd53ec909e4399b67c74cc7f4e883488035cfcdb90b685758fa8b34/setuptools-41.4.0-py2.py3-none-any.whl (580kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 59.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<2.1.0,>=2.0.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 66.5MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: opt-einsum, absl-py, termcolor, gast\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built opt-einsum absl-py termcolor gast\n",
      "Installing collected packages: grpcio, tensorflow-estimator, keras-preprocessing, opt-einsum, absl-py, google-pasta, astor, termcolor, keras-applications, gast, setuptools, protobuf, markdown, tensorboard, tensorflow\n",
      "  Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 opt-einsum-3.1.0 protobuf-3.10.0 setuptools-41.4.0 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0 termcolor-1.1.0\n",
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K    100% |████████████████████████████████| 378kB 18.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.7/site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: h5py in ./anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 2.9MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in ./anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5.0 in ./anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 21.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto>=2.32 in ./anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/49/cc8d900df85f69bce673510aacd0473aba958244516829d422720f584632/boto3-1.9.248-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 74.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in ./anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.3.9)\n",
      "Collecting botocore<1.13.0,>=1.12.248 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/5c/922cc8c2cdcb905e96bd73dc2064939bc9157e9e1d4c44e54e775cd2cddc/botocore-1.12.248-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.7MB 10.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 59.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in ./anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.248->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in ./anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.248->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.248 botocore-1.12.248 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.7/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install gensim\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NYASAboYK6Yz",
    "outputId": "82992754-0594-4af3-8dc7-d1cd80a3e5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm8eA0PlK1Ye"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmHpo7VuK1Yi",
    "outputId": "875676ae-8714-40d6-af6b-71fe02d122ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./anaconda3/lib/python3.7/site-packages (3.4)\r\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\r\n",
      "Requirement already satisfied: singledispatch in ./anaconda3/lib/python3.7/site-packages (from nltk) (3.4.0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "FVVGTjIXK1Yn",
    "outputId": "29489ec7-7d1c-40d0-862d-7f6834b35144"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gT1bja9LK1Yt",
    "outputId": "b35ab37a-2668-405b-aecd-ca395aef99eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meals can be served</td>\n",
       "      <td>in rooms at 9:00 p. m.</td>\n",
       "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>The local government can deal with the problem...</td>\n",
       "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The author called Tommy 's parents in order to</td>\n",
       "      <td>help them realize their influence on Tommy</td>\n",
       "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>'idioms are the most important part in a langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we deal with snake wounds according to...</td>\n",
       "      <td>Stay calm and do n't move .</td>\n",
       "      <td>'Cut the wound and suck the poison out .'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                Meals can be served   \n",
       "1           It can be inferred from the passage that   \n",
       "2     The author called Tommy 's parents in order to   \n",
       "3           It can be inferred from the passage that   \n",
       "4  How can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                             in rooms at 9:00 p. m.   \n",
       "1  The local government can deal with the problem...   \n",
       "2         help them realize their influence on Tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                        Stay calm and do n't move .   \n",
       "\n",
       "                                          distractor  \n",
       "0  'outside the room at 3:00 p. m.', 'in the dini...  \n",
       "1  'If some tragedies occur again ', ' relevant d...  \n",
       "2  'blame Tommy for his failing grades', 'blame T...  \n",
       "3  'idioms are the most important part in a langu...  \n",
       "4          'Cut the wound and suck the poison out .'  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_lmJheuDr8EW",
    "outputId": "43dcb6d5-39b2-4d77-8154-3da8ca5ba3e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'outside the room at 3:00 p. m.', 'in the dining - room at 6:00 p. m.', 'in the dining - room from 7:30 a. m. to 9:15 p. m.'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(df['distractor'])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "QO-NsrDWK1Yz",
    "outputId": "4bfac0a6-b797-453a-b1b3-1853cba6f816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What 'S the main idea of the text ?</td>\n",
       "      <td>The lack of career -- based courses in US high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the summer high season , Finland does nt se...</td>\n",
       "      <td>the sun is out at night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you want to apply for Chinese Business Inte...</td>\n",
       "      <td>have to get confirmed at least twice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That afternoon , the boy 's clothes were dry b...</td>\n",
       "      <td>nobody made room for him in the water .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements is NOT true ?</td>\n",
       "      <td>There are twelve countries in the World Wildli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                What 'S the main idea of the text ?   \n",
       "1  In the summer high season , Finland does nt se...   \n",
       "2  If you want to apply for Chinese Business Inte...   \n",
       "3  That afternoon , the boy 's clothes were dry b...   \n",
       "4    Which of the following statements is NOT true ?   \n",
       "\n",
       "                                         answer_text  \n",
       "0  The lack of career -- based courses in US high...  \n",
       "1                            the sun is out at night  \n",
       "2               have to get confirmed at least twice  \n",
       "3            nobody made room for him in the water .  \n",
       "4  There are twelve countries in the World Wildli...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftp = pd.read_csv('Test.csv')\n",
    "dftp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcCP83_t8quR"
   },
   "outputs": [],
   "source": [
    "questp = list(dftp['question'])\n",
    "anstp = list(dftp['answer_text'])\n",
    "test_tp = questp + anstp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEAZnpVtK1Y3"
   },
   "outputs": [],
   "source": [
    "questp = list(df['question'])\n",
    "anstp = list(df['answer_text'])\n",
    "test_tp = questp + anstp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk8e6W_PK1Y8"
   },
   "outputs": [],
   "source": [
    "ques = list(df['question'])\n",
    "ans = list(df['answer_text'])\n",
    "dist = list(df['distractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ka4mzqlhK1ZB",
    "outputId": "7c7439bb-1c31-4a7a-c3a5-03aa9125a450"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31499/31499 [00:00<00:00, 1085599.57it/s]\n"
     ]
    }
   ],
   "source": [
    "dist = [i.split(',') for i in tqdm(dist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPHkJMmRK1ZH"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    \"\"\"\n",
    "    Function to clean the given text\n",
    "    :param text: Sentence string\n",
    "    :return: Cleaned sentence string\n",
    "    \"\"\"\n",
    "    for punct in \"/-'\":\n",
    "        text = text.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "    for punct in '.!?,#$%\\()*+-/:\"\"'';=@\"\"[\\\\]^_`{|}~':\n",
    "        text = text.replace(punct, '')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[?|!|\\'|\"|#]',r' ',text)\n",
    "    text = re.sub(r'[.|,|)|(|\\|/]',r' ',text)        #Removing Punctuations\n",
    "    text = re.sub(r\"what’s\", \"what is \", text)\n",
    "    text = re.sub(r\"\\’s\", \" \", text)\n",
    "    text = re.sub(r\"\\’ve\", \" have \", text)\n",
    "    text = re.sub(r\"hasn’t\", \"has not \", text)\n",
    "    text = re.sub(r\"can’t\", \"cannot \", text)\n",
    "    text = re.sub(r\"n’t\", \" not \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am \", text)\n",
    "    text = re.sub(r\"\\’re\", \" are \", text)\n",
    "    text = re.sub(r\"\\’d\", \" would \", text)\n",
    "    text = re.sub(r\"\\’ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\’scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\d+\\d+', 'NUM', text)\n",
    "    text = text.replace(\"‘\",\"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"’\",'')\n",
    "    text = text.replace(\"‘\",\"\")\n",
    "    text = text.replace('''\"\"''','')\n",
    "    text = text.replace('''\"''','')\n",
    "    text = re.sub(r' s ', ' ', text)\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k9BUvGSFK1ZM",
    "outputId": "f0e8cca8-aa8a-411c-fcad-a93e4487e8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.7/site-packages (4.31.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "v6bZjdtnK1ZQ",
    "outputId": "a67bac20-05cf-4ea1-d89c-5644d8dc1b93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31499/31499 [00:00<00:00, 54334.22it/s]\n",
      "100%|██████████| 31499/31499 [00:00<00:00, 55511.97it/s]\n",
      "100%|██████████| 31499/31499 [00:01<00:00, 24481.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    words = word_tokenize(text)\n",
    "    clean_words = [re.sub(r'[^A-Za-z]', '', word) for word in words]\n",
    "    cleaned = ' '.join(clean_words)\n",
    "    text = cleaned.lower()\n",
    "    return text\n",
    "ques = [clean_text(i) for i in tqdm(ques)]\n",
    "ans = [clean_text(i) for i in tqdm(ans)]\n",
    "dist = [[clean_text(j) for j in i] for i in tqdm(dist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hvTKyQF1K1ZV",
    "outputId": "821fd55e-a5ca-4c0b-94ad-0cf6405b2e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at NUM p m',\n",
       " 'in the dining room at NUM p m',\n",
       " 'in the dining room from NUM a m to NUM p m']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ew1oio9yK1ZZ",
    "outputId": "4ffd4525-2c8c-4d6c-d9a2-45befaf752a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31499it [00:00, 1818182.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meals can be served in rooms at NUM p m'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb = []\n",
    "for i, j in tqdm(zip(ques, ans)):\n",
    "    comb.append(i+' ' +j)\n",
    "comb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrSSaahJK1Zg"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "glove = pickle.load(open('glove.42B.300d.pkl','rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJqTH7JxK1Zq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6vNpHhewK1Zz",
    "outputId": "3b60b888-238f-4923-896b-ec73349c0e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eee'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "for i in comb:\n",
    "    all_data.append(i)\n",
    "for i in dist:\n",
    "    for j in i:\n",
    "        all_data.append(j)\n",
    "all_data = test_tp + all_data\n",
    "\n",
    "all_data.append('sss')\n",
    "all_data.append('eee')\n",
    "print(len(all_data))\n",
    "all_data[165294]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "peZPSU27K1Z6",
    "outputId": "58d3ee61-594d-42c0-f9cc-825191899f18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(pd.Series(all_data))\n",
    "def pad_seq(comb, max_len):\n",
    "    X_tokenized= tk.texts_to_sequences(pd.Series(comb).fillna('UNK'))\n",
    "    X_comb = pad_sequences(X_tokenized, maxlen = max_len, padding = 'post', truncating= 'post')\n",
    "    return X_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CeuuXebRK1aD",
    "outputId": "b5cb0947-4b95-406e-9352-8ba76a828e49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24505"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.word_index['sss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHamwo4HK1aI"
   },
   "outputs": [],
   "source": [
    "a = [-1 for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXkaOsiCK1aS"
   },
   "outputs": [],
   "source": [
    "glove['sss'] = np.ones((1, 300))[0]\n",
    "glove['eee'] = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1bESzpgK1ai"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tk_new.pkl', 'wb') as f:\n",
    "    pickle.dump(tk, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNF5Oke0K1aq",
    "outputId": "7d5ab177-fe91-4ab7-b2ee-bb70e3ea929a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sss outside the room at NUM p m',\n",
       " 'sss in the dining room at NUM p m',\n",
       " 'sss in the dining room from NUM a m to NUM p m']"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_start = []\n",
    "for i in dist:\n",
    "    m = []\n",
    "    for j in i:\n",
    "        m.append('sss ' + j)\n",
    "    dist_start.append(m)\n",
    "dist_start[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw5cXrvcK1ax",
    "outputId": "aa7dc8f4-2c14-40b9-d5d5-ace7844650d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at NUM p m eee',\n",
       " 'in the dining room at NUM p m eee',\n",
       " 'in the dining room from NUM a m to NUM p m eee']"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_end = []\n",
    "for i in dist:\n",
    "    m = []\n",
    "    for j in i:\n",
    "        m.append(j + ' eee')\n",
    "    dist_end.append(m)\n",
    "dist_end[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72DAVWoJK1a4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V66oC7IK1a-"
   },
   "outputs": [],
   "source": [
    "qa = pad_seq(comb, 20)\n",
    "dist_seq_start = [[pad_seq(j, 20) for j in i] for i in dist_start]\n",
    "dist_seq_end = [[pad_seq(j, 20) for j in i] for i in dist_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6dShThaK1bF",
    "outputId": "4b63a22f-c17c-4f40-880a-929f42e08c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31499\n",
      "31499\n"
     ]
    }
   ],
   "source": [
    "print(len(qa))\n",
    "print(len(dist_seq_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-_OVgLFpK1bK",
    "outputId": "70860725-e102-4950-d5f3-e7cdd30810c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1653\n",
      "24507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "EMBEDDING_DIM = 300\n",
    "word_index = tk.word_index\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in glove: \n",
    "        embedding_matrix[i] = glove[word]\n",
    "    elif word.lower() in glove:\n",
    "        embedding_matrix[i] = glove[word.lower()]\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TRMdIE3K1bP"
   },
   "outputs": [],
   "source": [
    "x = [i for i in qa]\n",
    "d1_start = [i[0] for i in dist_seq_start]\n",
    "d2_start = [i[1] if len(i)>=2 else np.zeros((1, 20)) for i in dist_seq_start]\n",
    "d3_start = [i[2] if len(i)>=3 else np.zeros((1, 20)) for i in dist_seq_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6NS8eLkK1bW"
   },
   "outputs": [],
   "source": [
    "d1_end = [i[0] for i in dist_seq_end]\n",
    "d2_end = [i[1] if len(i)>=2 else np.zeros((1, 20)) for i in dist_seq_end]\n",
    "d3_end = [i[2] if len(i)>=3 else np.zeros((1, 20)) for i in dist_seq_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpeH8k9fK1bf",
    "outputId": "8d2125c2-2ca3-4a6f-a58d-98b2acd26a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[   90    17   119   270     2  1680  4486 24506     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(d3_start[10])\n",
    "print(d3_end[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlYlCdvCK1bn",
    "outputId": "25471117-415c-41cf-85ca-dfca8768526e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31499\n",
      "31499\n",
      "31499\n",
      "31499\n",
      "31499\n"
     ]
    }
   ],
   "source": [
    "print(len(dist))\n",
    "print(len(x))\n",
    "print(len(d1_start))\n",
    "print(len(d2_end))\n",
    "print(len(d3_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eu004iBPK1bv",
    "outputId": "0edfaae7-3a55-4590-edeb-773cc1aa5802"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31499, 1, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "d1_start = np.array(d1_start)\n",
    "d2_start = np.array(d1_start)\n",
    "d3_start = np.array(d1_start)\n",
    "d3_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMALisg6K1b3",
    "outputId": "0ff123b7-11d9-4850-84ca-dbe39d1fa941"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31499, 1, 20)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1_end = np.array(d1_end)\n",
    "d2_end = np.array(d1_end)\n",
    "d3_end = np.array(d1_end)\n",
    "d3_end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0kDCKkHK1cD",
    "outputId": "5af57851-f7ba-4d6f-d1d2-0168a77fe4dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  import sys\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if sys.path[0] == '':\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "trainx = np.zeros((31499, 20, 300))\n",
    "for i in range(31499):\n",
    "    trainx[i] = embedding_matrix[[x[i]]]\n",
    "\n",
    "traind1s = np.zeros((31499, 20, 300))\n",
    "for i in range(31499):\n",
    "    traind1s[i] = embedding_matrix[[d1_start[i]]]\n",
    "\n",
    "traind2s = np.zeros((31499, 20, 300))\n",
    "for i in range(31499):\n",
    "    try:\n",
    "        traind2s[i] = embedding_matrix[[d2_start[i]]]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "traind3s = np.zeros((31499, 20, 300))\n",
    "for i in range(31499):\n",
    "    try:\n",
    "        traind3s[i] = embedding_matrix[[d3_start[i]]]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvPF_2JwK1cJ",
    "outputId": "582fb314-7fe0-4fb8-bf24-58351f2a1a5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24507"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sOPC7WmJK1cP"
   },
   "outputs": [],
   "source": [
    "inp = 0\n",
    "model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pV1tG373K1cV",
    "outputId": "360dbd51-80f8-4197-dbc5-31bd9ee0b224"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20, 24507)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils as ku\n",
    "traind1e = ku.to_categorical(d1_end[10000:20000], num_classes=24507)\n",
    "traind2e = ku.to_categorical(d2_end[10000:20000], num_classes=24507)\n",
    "traind3e = ku.to_categorical(d3_end[10000:20000], num_classes=24507)\n",
    "traind1e = traind1e.reshape(10000, 20, 24507)\n",
    "traind2e = traind2e.reshape(10000, 20, 24507)\n",
    "traind3e = traind3e.reshape(10000, 20, 24507)\n",
    "traind1e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-GRmBItK1cd",
    "outputId": "9495482a-6fb9-48bf-9f02-48f6573964d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4, 20, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = []\n",
    "for i, j, k, l in zip(trainx[10000:20000], traind1s[10000:20000], traind2s[10000:20000], traind3s[10000:20000]):\n",
    "    inp.append([i, j, k, l])\n",
    "inp = np.array(inp).reshape(10000, 4, 20, 300)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2G8BjeeK1ch"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Input, Dropout, Flatten, Embedding, BatchNormalization, Concatenate, concatenate, Add, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7PtiCIiSK1cp",
    "outputId": "e8acafb6-8b00-4257-ec8f-e69ab1eea592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "Kk_6p3xSK1cv",
    "outputId": "ed43a6f3-ac42-4431-8cca-b63b95c17dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    #inpQA = inp[0]\n",
    "    inpQA = Input(shape=(4, 20, 300))\n",
    "    encoder = LSTM(300, return_state=True)\n",
    "    x1 = Lambda(lambda x:x[:, 0, :])(inpQA)\n",
    "    print(x1.shape)\n",
    "    encoder_outputs, state_h, state_c = encoder(x1)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_lstm1 = LSTM(300, return_sequences=True, return_state = True)\n",
    "    x2 = Lambda(lambda x:x[:, 1, :])(inpQA)\n",
    "    decoder_outputs1, _, _  = decoder_lstm1(x2,\n",
    "                                      initial_state=encoder_states)\n",
    "    decoder_lstm2 = LSTM(300, return_sequences=True, return_state = True)\n",
    "    x3 = Lambda(lambda x:x[:, 2, :])(inpQA)\n",
    "\n",
    "    decoder_outputs2,  _, _ = decoder_lstm2(x3,\n",
    "                                        initial_state=encoder_states)\n",
    "    decoder_lstm3 = LSTM(300, return_sequences=True, return_state = True)\n",
    "    x4 = Lambda(lambda x:x[:, 3, :])(inpQA)\n",
    "    decoder_outputs3, _, _  = decoder_lstm3(x4,\n",
    "                                        initial_state=encoder_states)\n",
    "\n",
    "    o1 = Dense(24507, activation = 'softmax')(decoder_outputs1)\n",
    "    o2 = Dense(24507, activation = 'softmax')(decoder_outputs2)\n",
    "    o3 = Dense(24507, activation = 'softmax')(decoder_outputs3)\n",
    "                                        \n",
    "    cat_conv = [o1, o2, o3]\n",
    "    \n",
    "\n",
    "    model = Model(inputs=inpQA, outputs = cat_conv)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbld1NlRK1cy"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"/content/drive/My Drive/ML/Valuelabs/temp.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgGoFWkoK1c3",
    "outputId": "b1f10793-6e5f-429b-8409-6814308c6df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10000/10000 [==============================] - 267s 27ms/step - loss: 6.2034 - dense_4_loss: 2.0679 - dense_5_loss: 2.0665 - dense_6_loss: 2.0657 - dense_4_accuracy: 0.6976 - dense_5_accuracy: 0.6978 - dense_6_accuracy: 0.6983\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 258s 26ms/step - loss: 5.4855 - dense_4_loss: 1.8261 - dense_5_loss: 1.8276 - dense_6_loss: 1.8275 - dense_4_accuracy: 0.7087 - dense_5_accuracy: 0.7077 - dense_6_accuracy: 0.7084\n",
      "Epoch 3/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 4.9777 - dense_4_loss: 1.6558 - dense_5_loss: 1.6615 - dense_6_loss: 1.6619 - dense_4_accuracy: 0.7167 - dense_5_accuracy: 0.7162 - dense_6_accuracy: 0.7164\n",
      "Epoch 4/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 4.5051 - dense_4_loss: 1.4972 - dense_5_loss: 1.5050 - dense_6_loss: 1.5049 - dense_4_accuracy: 0.7259 - dense_5_accuracy: 0.7253 - dense_6_accuracy: 0.7255\n",
      "Epoch 5/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 4.0545 - dense_4_loss: 1.3444 - dense_5_loss: 1.3538 - dense_6_loss: 1.3547 - dense_4_accuracy: 0.7383 - dense_5_accuracy: 0.7372 - dense_6_accuracy: 0.7366\n",
      "Epoch 6/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 3.6294 - dense_4_loss: 1.2021 - dense_5_loss: 1.2124 - dense_6_loss: 1.2129 - dense_4_accuracy: 0.7549 - dense_5_accuracy: 0.7533 - dense_6_accuracy: 0.7531\n",
      "Epoch 7/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 3.2371 - dense_4_loss: 1.0716 - dense_5_loss: 1.0814 - dense_6_loss: 1.0835 - dense_4_accuracy: 0.7767 - dense_5_accuracy: 0.7753 - dense_6_accuracy: 0.7741\n",
      "Epoch 8/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 2.8868 - dense_4_loss: 0.9552 - dense_5_loss: 0.9656 - dense_6_loss: 0.9678 - dense_4_accuracy: 0.7993 - dense_5_accuracy: 0.7979 - dense_6_accuracy: 0.7961\n",
      "Epoch 9/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 2.5839 - dense_4_loss: 0.8538 - dense_5_loss: 0.8642 - dense_6_loss: 0.8671 - dense_4_accuracy: 0.8209 - dense_5_accuracy: 0.8193 - dense_6_accuracy: 0.8169\n",
      "Epoch 10/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 2.3135 - dense_4_loss: 0.7632 - dense_5_loss: 0.7741 - dense_6_loss: 0.7767 - dense_4_accuracy: 0.8398 - dense_5_accuracy: 0.8379 - dense_6_accuracy: 0.8361\n",
      "Epoch 11/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 2.0766 - dense_4_loss: 0.6852 - dense_5_loss: 0.6945 - dense_6_loss: 0.6979 - dense_4_accuracy: 0.8572 - dense_5_accuracy: 0.8553 - dense_6_accuracy: 0.8540\n",
      "Epoch 12/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 1.8650 - dense_4_loss: 0.6127 - dense_5_loss: 0.6235 - dense_6_loss: 0.6270 - dense_4_accuracy: 0.8735 - dense_5_accuracy: 0.8715 - dense_6_accuracy: 0.8710\n",
      "Epoch 13/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 1.6720 - dense_4_loss: 0.5498 - dense_5_loss: 0.5602 - dense_6_loss: 0.5632 - dense_4_accuracy: 0.8888 - dense_5_accuracy: 0.8870 - dense_6_accuracy: 0.8856\n",
      "Epoch 14/15\n",
      "10000/10000 [==============================] - 258s 26ms/step - loss: 1.5010 - dense_4_loss: 0.4926 - dense_5_loss: 0.5024 - dense_6_loss: 0.5064 - dense_4_accuracy: 0.9027 - dense_5_accuracy: 0.9002 - dense_6_accuracy: 0.8995\n",
      "Epoch 15/15\n",
      "10000/10000 [==============================] - 259s 26ms/step - loss: 1.3448 - dense_4_loss: 0.4406 - dense_5_loss: 0.4511 - dense_6_loss: 0.4539 - dense_4_accuracy: 0.9163 - dense_5_accuracy: 0.9126 - dense_6_accuracy: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fca9640cfd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inp, [traind1e, traind2e, traind3e], batch_size=64, epochs=15, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rz0SVrExK1c8"
   },
   "outputs": [],
   "source": [
    "model.save('VL2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "8ZC671ojK1dA",
    "outputId": "f467aa7f-3099-481a-e7cc-ea17eaa91771"
   },
   "outputs": [],
   "source": [
    "model.load_weights('VL2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eJ6c8SVtJWu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvH-LhOYK1dI"
   },
   "outputs": [],
   "source": [
    "def get_seq(sentence, count):\n",
    "    seqq = tk.texts_to_sequences(pd.Series(sentence).fillna('UNK'))\n",
    "    seq_vec = pad_sequences(seqq, maxlen = 20, padding = 'post', truncating= 'post')\n",
    "    emb_vec_qa = embedding_matrix[[seq_vec]]\n",
    "    emb_vec_qa = emb_vec_qa.reshape(20, 300)\n",
    "    d1 = [0 for i in range(20)]\n",
    "    d1 = np.array(d1)\n",
    "    d1[0] = tk.word_index['sss']\n",
    "    d1 = embedding_matrix[[d1]]\n",
    "    d1 = d1.reshape(20, 300)\n",
    "    d2 = [0 for i in range(20)]\n",
    "    d2 = np.array(d2)\n",
    "    d2[0] = tk.word_index['sss']\n",
    "    d2 = embedding_matrix[[d2]]\n",
    "    d2 = d2.reshape(20, 300)\n",
    "    d3 = [0 for i in range(20)]\n",
    "    d3 = np.array(d3)\n",
    "    d3[0] = tk.word_index['sss']\n",
    "    d3 = embedding_matrix[[d3]]\n",
    "    d3 = d3.reshape(20, 300)\n",
    "    inn = []\n",
    "    inn.append([emb_vec_qa, d1, d2, d3])\n",
    "    inn = np.array(inn)\n",
    "    inn = inn.reshape(1, 4, 20, 300)\n",
    "    flag = True\n",
    "    predict1 = []\n",
    "    predict2 = []\n",
    "    predict3 = []\n",
    "    c = 0\n",
    "    while flag:\n",
    "        d1p = np.argmax(model.predict(inn)[0][0][0])\n",
    "        mydict = tk.word_index\n",
    "        if d1p==0:\n",
    "            pred1 = ''\n",
    "            break\n",
    "        word = list(mydict.keys())[list(mydict.values()).index(d1p)]\n",
    "\n",
    "        d1 = [0 for i in range(20)]\n",
    "        d1 = np.array(d1)\n",
    "        d1[0] = tk.word_index[word]\n",
    "        d1 = embedding_matrix[[d1]]\n",
    "        d1 = d1.reshape(20, 300)\n",
    "        inn = []\n",
    "        inn.append([emb_vec_qa, d1, d2, d3])\n",
    "        inn = np.array(inn)\n",
    "        inn = inn.reshape(1, 4, 20, 300)\n",
    "        out = []\n",
    "        c+=1\n",
    "        if word == 'eee' or c > count:\n",
    "            flag = False\n",
    "            pred1 = ' '.join(predict1)\n",
    "        predict1.append(word)\n",
    "\n",
    "\n",
    "\n",
    "#2nd one\n",
    "    c = 0\n",
    "    flag = True\n",
    "    while flag:\n",
    "        d2p = np.argmax(model.predict(inn)[1][0][0])\n",
    "        mydict = tk.word_index\n",
    "        if d2p==0:\n",
    "            pred2 =  ''\n",
    "            break\n",
    "        word = list(mydict.keys())[list(mydict.values()).index(d2p)]\n",
    "\n",
    "        d2 = [0 for i in range(20)]\n",
    "        d2 = np.array(d2)\n",
    "        d2[0] = tk.word_index[word]\n",
    "        d2 = embedding_matrix[[d2]]\n",
    "        d2 = d2.reshape(20, 300)\n",
    "        inn = []\n",
    "        inn.append([emb_vec_qa, d1, d2, d3])\n",
    "        inn = np.array(inn)\n",
    "        inn = inn.reshape(1, 4, 20, 300)\n",
    "        out = []\n",
    "        c+=1\n",
    "        if word == 'eee' or c > count:\n",
    "            flag = False\n",
    "            pred2 =' '.join(predict2)          \n",
    "        predict2.append(word)\n",
    "\n",
    "#third=================\n",
    "    c = 0\n",
    "    flag = True\n",
    "    while flag:\n",
    "        d3p = np.argmax(model.predict(inn)[2][0][0])\n",
    "        mydict = tk.word_index\n",
    "        if d3p==0:\n",
    "            pred3 =  ''\n",
    "            break\n",
    "        word = list(mydict.keys())[list(mydict.values()).index(d3p)]\n",
    "\n",
    "        d3 = [0 for i in range(20)]\n",
    "        d3 = np.array(d3)\n",
    "        d3[0] = tk.word_index[word]\n",
    "        d3 = embedding_matrix[[d3]]\n",
    "        d3 = d3.reshape(20, 300)\n",
    "        inn = []\n",
    "        inn.append([emb_vec_qa, d1, d2, d3])\n",
    "        inn = np.array(inn)\n",
    "        inn = inn.reshape(1, 4, 20, 300)\n",
    "        out = []\n",
    "        c+=1\n",
    "        if word == 'eee' or c > count:\n",
    "            flag = False\n",
    "            pred3 = ' '.join(predict3)\n",
    "        predict3.append(word)\n",
    "\n",
    "    return f\"'{pred1}', '{pred2}', '{pred3}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vsXiGZ5Euksy",
    "outputId": "3abf96cd-8fe6-407d-ba4b-88e67ec7c764"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What 'S the main idea of the text ?</td>\n",
       "      <td>The lack of career -- based courses in US high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the summer high season , Finland does nt se...</td>\n",
       "      <td>the sun is out at night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you want to apply for Chinese Business Inte...</td>\n",
       "      <td>have to get confirmed at least twice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That afternoon , the boy 's clothes were dry b...</td>\n",
       "      <td>nobody made room for him in the water .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements is NOT true ?</td>\n",
       "      <td>There are twelve countries in the World Wildli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                What 'S the main idea of the text ?   \n",
       "1  In the summer high season , Finland does nt se...   \n",
       "2  If you want to apply for Chinese Business Inte...   \n",
       "3  That afternoon , the boy 's clothes were dry b...   \n",
       "4    Which of the following statements is NOT true ?   \n",
       "\n",
       "                                         answer_text  \n",
       "0  The lack of career -- based courses in US high...  \n",
       "1                            the sun is out at night  \n",
       "2               have to get confirmed at least twice  \n",
       "3            nobody made room for him in the water .  \n",
       "4  There are twelve countries in the World Wildli...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftp = pd.read_csv('Test.csv')\n",
    "dftp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4x5WKAyguJSf"
   },
   "outputs": [],
   "source": [
    "q = list(dftp['question'])\n",
    "an = list(dftp['answer_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "J30c09-38H-q",
    "outputId": "012ab3db-ee39-467b-8a01-9168a0150741"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13500it [00:00, 1690797.10it/s]\n",
      "100%|██████████| 13500/13500 [00:00<00:00, 48439.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what the main idea of the text the lack of career based courses in us high schools'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb1 = []\n",
    "for i, j in tqdm(zip(q, an)):\n",
    "    comb1.append(i+' ' +j)\n",
    "comb1 = [clean_text(i) for i in tqdm(comb1)]\n",
    "comb1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSeR54j4vELg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13500 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:70: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:97: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "100%|██████████| 13500/13500 [1:15:32<00:00,  3.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'were the man could he wanted his man could he', 'the woman did n t the woman did n t', 'their knew were too was too was too was too'\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis1 = [get_seq(i, 10) for i in tqdm(comb1)]\n",
    "dis1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('Results.csv')\n",
    "df_new['distractor'] = dis1\n",
    "df_new.to_csv('Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(df['distractor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['idioms are the most important part in a language', 'nonnative speakers should learn more idioms', 'there are no ways to master idioms']\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(dist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'outside the room at 3:00 p. m.', 'in the dining - room at 6:00 p. m.', 'in the dining - room from 7:30 a. m. to 9:15 p. m.'\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "5fubUFtkK1dP",
    "outputId": "db3a0296-535f-4696-ade9-0165aa598c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:97: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the homeless children in the homeless children in the homeless children in',\n",
       " 'the people will be important in two men have two men have',\n",
       " 'the children can be too much is too much is too much']"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq('One part of the homeless population is difficult to estimate . The reason might well be some homeless children are deserted by their families', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKIDFCzIMpaN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "distractorNews111.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
